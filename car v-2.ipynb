{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36c68146",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Author: Sonu Gupta\n",
    "    Project: Training a car racing machine learning model\n",
    "\"\"\"    \n",
    "\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce87274d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(5)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CarRacing-v2\", continuous = False)\n",
    "state_size = 96 * 96 * 3\n",
    "action_size = 5\n",
    "batch_size = 15\n",
    "n_episodes = 5\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed6a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=500)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.95\n",
    "        self.epsilon_min = 0.05\n",
    "       \n",
    "        self.learning_rate = 0.001 # This should be lower (0.001?)\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def process_state(state):\n",
    "        state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)\n",
    "        state = state.astype(float)\n",
    "        state /= 255.0\n",
    "        return state\n",
    "    \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(12, activation = \"relu\", input_dim=self.state_size))\n",
    "        model.add(Dense(12, activation = \"relu\"))\n",
    "        model.add(Dense(self.action_size, activation = \"linear\"))\n",
    "        model.compile(loss=\"mse\", optimizer = Adam(learning_rate = self.learning_rate))\n",
    "        return model\n",
    "   \n",
    "    def remember(self, s, a, r, s_prime, done):\n",
    "        self.memory.append((s, a, r, s_prime, done))\n",
    "       \n",
    "    def train(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for s, a, r, s_prime, done in minibatch:\n",
    "            target = r # if done is true (terminal state)\n",
    "            if not done:\n",
    "                target = (r + self.gamma * np.amax(self.model.predict(s_prime)[0]))\n",
    "            target_f = self.model.predict(s)\n",
    "            target_f[0][a] = target\n",
    "            self.model.fit(s, target_f, epochs=1, verbose=False)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "   \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "   \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "   \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2660505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e44a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(n_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, (-1, 27648))\n",
    "    #agent.process_state(state)\n",
    "    for time in range(200):\n",
    "        #env.render()\n",
    "        action = agent.act(state)\n",
    "        # print(action)\n",
    "        next_state, reward, done, trunc, _ = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, (-1, 27648))\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done or trunc:\n",
    "            print(f\"episode: {e}/{n_episodes}, score: {time}, e: {agent.epsilon}\")\n",
    "            break\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.train(batch_size)\n",
    "        if e % 10 == 0:\n",
    "            agent.save(\"./model_output/CarRacing-v2\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
